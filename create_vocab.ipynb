{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\".\")\n",
    "data_dir = PATH.joinpath(\"normalization_dataset\")\n",
    "train_clean_path = data_dir.joinpath(\"normalization_train.1blm\").absolute()\n",
    "train_corrupt_path = data_dir.joinpath(\n",
    "    \"normalization_train.1blm.noise.random\"\n",
    ").absolute()\n",
    "test_clean_path = data_dir.joinpath(\"normalization_test.1blm\").absolute()\n",
    "test_corrupt_path = data_dir.joinpath(\"normalization_test.1blm.noise.random\").absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_path, corr_file, incorr_file):\n",
    "    # load files\n",
    "    if base_path:\n",
    "        assert os.path.exists(base_path)\n",
    "    incorr_data = []\n",
    "    opfile1 = open(os.path.join(base_path, incorr_file), \"r\")\n",
    "    for line in opfile1:\n",
    "        if line.strip() != \"\":\n",
    "            incorr_data.append(line.strip())\n",
    "    opfile1.close()\n",
    "    corr_data = []\n",
    "    opfile2 = open(os.path.join(base_path, corr_file), \"r\")\n",
    "    for line in opfile2:\n",
    "        if line.strip() != \"\":\n",
    "            corr_data.append(line.strip())\n",
    "    opfile2.close()\n",
    "    assert len(incorr_data) == len(corr_data)\n",
    "\n",
    "    # verify if token split is same\n",
    "    for i, (x, y) in tqdm(enumerate(zip(corr_data, incorr_data))):\n",
    "        x_split, y_split = x.split(), y.split()\n",
    "        try:\n",
    "            assert len(x_split) == len(y_split)\n",
    "        except AssertionError:\n",
    "            print(\n",
    "                \"# tokens in corr and incorr mismatch. retaining and trimming to min len.\"\n",
    "            )\n",
    "            print(x_split, y_split)\n",
    "            mn = min([len(x_split), len(y_split)])\n",
    "            corr_data[i] = \" \".join(x_split[:mn])\n",
    "            incorr_data[i] = \" \".join(y_split[:mn])\n",
    "            print(corr_data[i], incorr_data[i])\n",
    "\n",
    "    # return as pairs\n",
    "    data = []\n",
    "    for x, y in tqdm(zip(corr_data, incorr_data)):\n",
    "        data.append((x, y))\n",
    "\n",
    "    print(f\"loaded tuples of (corr,incorr) examples from {base_path}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_tokens(use_default: bool, data=None):\n",
    "    if not use_default and data is None:\n",
    "        raise Exception(\"data is None\")\n",
    "\n",
    "    # reset char token utils\n",
    "    chartoken2idx, idx2chartoken = {}, {}\n",
    "    # (\n",
    "    #     char_unk_token,\n",
    "    #     char_pad_token,\n",
    "    #     char_start_token,\n",
    "    #     char_end_token,\n",
    "    #     char_mask_token,\n",
    "    # ) = \"<<CHAR_UNK>>\", \"<<CHAR_PAD>>\", \"<<CHAR_START>>\", \"<<CHAR_END>>\", \"<<CHAR_MAK>>\"\n",
    "    # special_tokens = [\n",
    "    #     char_unk_token,\n",
    "    #     char_pad_token,\n",
    "    #     char_start_token,\n",
    "    #     char_end_token,\n",
    "    #     char_mask_token,\n",
    "    # ]\n",
    "    # for char in special_tokens:\n",
    "    #     idx = len(chartoken2idx)\n",
    "    #     chartoken2idx[char] = idx\n",
    "    #     idx2chartoken[idx] = char\n",
    "\n",
    "    if use_default:\n",
    "        chars = list(\n",
    "            \"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\"\n",
    "        )\n",
    "        for char in chars:\n",
    "            if char not in chartoken2idx:\n",
    "                idx = len(chartoken2idx)\n",
    "                chartoken2idx[char] = idx\n",
    "                idx2chartoken[idx] = char\n",
    "    else:\n",
    "        # helper funcs\n",
    "        # isascii = lambda s: len(s) == len(s.encode())\n",
    "        \"\"\"\n",
    "        # load batches of lines and obtain unique chars\n",
    "        nlines = len(data)\n",
    "        bsize = 5000\n",
    "        nbatches = int( np.ceil(nlines/bsize) )\n",
    "        for i in tqdm(range(nbatches)):\n",
    "            blines = \" \".join( [ex for ex in data[i*bsize:(i+1)*bsize]] )\n",
    "            #bchars = set(list(blines))\n",
    "            for char in bchars:\n",
    "                if char not in chartoken2idx:\n",
    "                    idx = len(chartoken2idx)\n",
    "                    chartoken2idx[char] = idx\n",
    "                    idx2chartoken[idx] = char\n",
    "        \"\"\"\n",
    "        # realized that set doesn't preserve order!!\n",
    "        for line in tqdm(data):\n",
    "            for char in line:  # type: ignore\n",
    "                if char not in chartoken2idx:\n",
    "                    idx = len(chartoken2idx)\n",
    "                    chartoken2idx[char] = idx\n",
    "                    idx2chartoken[idx] = char\n",
    "        (\n",
    "            char_unk_token,\n",
    "            char_pad_token,\n",
    "            char_start_token,\n",
    "            char_end_token,\n",
    "            char_mask_token,\n",
    "        ) = (\n",
    "            \"<<CHAR_UNK>>\",\n",
    "            \"<<CHAR_PAD>>\",\n",
    "            \"<<CHAR_START>>\",\n",
    "            \"<<CHAR_END>>\",\n",
    "            \"<<CHAR_MAK>>\",\n",
    "        )\n",
    "        special_tokens = [\n",
    "            char_unk_token,\n",
    "            char_pad_token,\n",
    "            char_start_token,\n",
    "            char_end_token,\n",
    "            char_mask_token,\n",
    "        ]\n",
    "        for char in special_tokens:\n",
    "            idx = len(chartoken2idx)\n",
    "            chartoken2idx[char] = idx\n",
    "            idx2chartoken[idx] = char\n",
    "\n",
    "    print(f\"number of unique chars found: {len(chartoken2idx)}\")\n",
    "    print(chartoken2idx)\n",
    "    return_dict = {}\n",
    "    return_dict[\"chartoken2idx\"] = chartoken2idx\n",
    "    return_dict[\"idx2chartoken\"] = idx2chartoken\n",
    "    return_dict[\"char_unk_token\"] = char_unk_token\n",
    "    return_dict[\"char_pad_token\"] = char_pad_token\n",
    "    return_dict[\"char_start_token\"] = char_start_token\n",
    "    return_dict[\"char_end_token\"] = char_end_token\n",
    "    return_dict[\"char_maks_token\"] = char_mask_token\n",
    "    # new\n",
    "    return_dict[\"char_unk_token_idx\"] = chartoken2idx[char_unk_token]\n",
    "    return_dict[\"char_pad_token_idx\"] = chartoken2idx[char_pad_token]\n",
    "    return_dict[\"char_start_token_idx\"] = chartoken2idx[char_start_token]\n",
    "    return_dict[\"char_end_token_idx\"] = chartoken2idx[char_end_token]\n",
    "    return_dict[\"char_mask_token_idx\"] = chartoken2idx[char_mask_token]\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(\n",
    "    data,\n",
    "    keep_simple=False,\n",
    "    min_max_freq=(1, float(\"inf\")),\n",
    "    topk=None,\n",
    "    intersect=[],\n",
    "    load_char_tokens=False,\n",
    "):\n",
    "    # get all tokens\n",
    "    token_freq, token2idx, idx2token = {}, {}, {}\n",
    "    for example in tqdm(data):  ##########################\n",
    "        for token in example.split():\n",
    "            if token not in token_freq:\n",
    "                token_freq[token] = 0\n",
    "            token_freq[token] += 1\n",
    "    print(f\"Total tokens found: {len(token_freq)}\")\n",
    "\n",
    "    # retain only simple tokens\n",
    "    if keep_simple:\n",
    "\n",
    "        def isascii(s):\n",
    "            return len(s) == len(s.encode())\n",
    "\n",
    "        def hasdigits(s):\n",
    "            return len([x for x in list(s) if x.isdigit()]) > 0\n",
    "\n",
    "        tf = [\n",
    "            (t, f)\n",
    "            for t, f in [*token_freq.items()]\n",
    "            if (isascii(t) and not hasdigits(t))\n",
    "        ]\n",
    "        token_freq = {t: f for (t, f) in tf}\n",
    "        print(f\"Total tokens retained: {len(token_freq)}\")\n",
    "\n",
    "    # retain only tokens with specified min and max range\n",
    "    if min_max_freq[0] > 1 or min_max_freq[1] < float(\"inf\"):\n",
    "        sorted_ = sorted(token_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "        tf = [\n",
    "            (i[0], i[1])\n",
    "            for i in sorted_\n",
    "            if (i[1] >= min_max_freq[0] and i[1] <= min_max_freq[1])\n",
    "        ]\n",
    "        token_freq = {t: f for (t, f) in tf}\n",
    "        print(f\"Total tokens retained: {len(token_freq)}\")\n",
    "\n",
    "    # retain only topk tokens\n",
    "    if topk is not None:\n",
    "        sorted_ = sorted(token_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "        token_freq = {t: f for (t, f) in list(sorted_)[:topk]}\n",
    "        print(f\"Total tokens retained: {len(token_freq)}\")\n",
    "\n",
    "    # retain only interection of tokens\n",
    "    if len(intersect) > 0:\n",
    "        tf = [\n",
    "            (t, f)\n",
    "            for t, f in [*token_freq.items()]\n",
    "            if (t in intersect or t.lower() in intersect)\n",
    "        ]\n",
    "        token_freq = {t: f for (t, f) in tf}\n",
    "        print(f\"Total tokens retained: {len(token_freq)}\")\n",
    "\n",
    "    # create token2idx and idx2token\n",
    "    for token in token_freq:\n",
    "        idx = len(token2idx)\n",
    "        idx2token[idx] = token\n",
    "        token2idx[token] = idx\n",
    "\n",
    "    # add <<PAD>> special token\n",
    "    ntokens = len(token2idx)\n",
    "    pad_token = \"<<PAD>>\"\n",
    "    token_freq.update({pad_token: -1})\n",
    "    token2idx.update({pad_token: ntokens})\n",
    "    idx2token.update({ntokens: pad_token})\n",
    "\n",
    "    # add <<UNK>> special token\n",
    "    ntokens = len(token2idx)\n",
    "    unk_token = \"<<UNK>>\"\n",
    "    token_freq.update({unk_token: -1})\n",
    "    token2idx.update({unk_token: ntokens})\n",
    "    idx2token.update({ntokens: unk_token})\n",
    "\n",
    "    # new\n",
    "    # add <<EOS>> special token\n",
    "    ntokens = len(token2idx)\n",
    "    eos_token = \"<<EOS>>\"\n",
    "    token_freq.update({eos_token: -1})\n",
    "    token2idx.update({eos_token: ntokens})\n",
    "    idx2token.update({ntokens: eos_token})\n",
    "\n",
    "    # new\n",
    "    # add <<MAK>> special token\n",
    "    ntokens = len(token2idx)\n",
    "    mak_token = \"<<MAK>>\"\n",
    "    token_freq.update({mak_token: -1})\n",
    "    token2idx.update({mak_token: ntokens})\n",
    "    idx2token.update({ntokens: mak_token})\n",
    "\n",
    "    # return dict\n",
    "    token_freq = list(\n",
    "        sorted(token_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    return_dict = {\n",
    "        \"token2idx\": token2idx,\n",
    "        \"idx2token\": idx2token,\n",
    "        \"token_freq\": token_freq,\n",
    "        \"pad_token\": pad_token,\n",
    "        \"unk_token\": unk_token,\n",
    "        \"eos_token\": eos_token,\n",
    "        \"mak_token\": mak_token,\n",
    "    }\n",
    "    # new\n",
    "    return_dict.update(\n",
    "        {\n",
    "            \"pad_token_idx\": token2idx[pad_token],\n",
    "            \"unk_token_idx\": token2idx[unk_token],\n",
    "            \"eos_token_idx\": token2idx[eos_token],\n",
    "            \"mak_token_idx\": token2idx[mak_token],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # load_char_tokens\n",
    "    if load_char_tokens:\n",
    "        print(\"loading character tokens\")\n",
    "        char_return_dict = get_char_tokens(use_default=False, data=data)\n",
    "        return_dict.update(char_return_dict)\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab_dict(path_: str, vocab_: dict):\n",
    "    \"\"\"\n",
    "    path_: path where the vocab pickle file to be saved\n",
    "    vocab_: the dict data\n",
    "    \"\"\"\n",
    "    with open(path_, \"wb\") as fp:\n",
    "        pickle.dump(vocab_, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1365633it [00:03, 453329.29it/s]\n",
      "1365633it [00:00, 1856858.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tuples of (corr,incorr) examples from normalization_dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "273129it [00:00, 463866.88it/s]\n",
      "273129it [00:00, 1810853.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tuples of (corr,incorr) examples from normalization_dataset\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365633/1365633 [00:07<00:00, 188212.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens found: 266611\n",
      "Total tokens retained: 100000\n",
      "loading character tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1365633/1365633 [00:04<00:00, 327430.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique chars found: 62\n",
      "{'H': 0, 'e': 1, ' ': 2, 'i': 3, 's': 4, 'a': 5, 'b': 6, 'g': 7, 'p': 8, 'l': 9, 'y': 10, 'r': 11, 'f': 12, 'o': 13, 't': 14, 'h': 15, 'c': 16, 'u': 17, 'n': 18, 'd': 19, ',': 20, 'm': 21, '.': 22, 'W': 23, 'M': 24, 'R': 25, 'A': 26, 'w': 27, 'k': 28, 'I': 29, 'C': 30, 'v': 31, 'j': 32, 'V': 33, 'z': 34, \"'\": 35, 'Z': 36, 'B': 37, 'Y': 38, 'P': 39, 'L': 40, 'N': 41, 'x': 42, 'D': 43, 'S': 44, 'F': 45, 'T': 46, 'G': 47, 'E': 48, 'O': 49, 'U': 50, 'X': 51, 'J': 52, 'K': 53, 'q': 54, ';': 55, 'Q': 56, '<<CHAR_UNK>>': 57, '<<CHAR_PAD>>': 58, '<<CHAR_START>>': 59, '<<CHAR_END>>': 60, '<<CHAR_MAK>>': 61}\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "train_data = load_data(data_dir, train_clean_path, train_corrupt_path)\n",
    "# test_data = load_data(data_dir, test_clean_path, test_corrupt_path)\n",
    "print(\"------------------------\")\n",
    "clean_sentences = [sentence_pair[0] for sentence_pair in train_data]\n",
    "corrupt_sentences = [sentence_pair[1] for sentence_pair in train_data]\n",
    "voc = get_tokens(\n",
    "    clean_sentences,\n",
    "    keep_simple=False,\n",
    "    min_max_freq=(1, float(\"inf\")),\n",
    "    topk=100000,\n",
    "    load_char_tokens=True,\n",
    ")\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token2idx', 'idx2token', 'token_freq', 'pad_token', 'unk_token', 'eos_token', 'mak_token', 'pad_token_idx', 'unk_token_idx', 'eos_token_idx', 'mak_token_idx', 'chartoken2idx', 'idx2chartoken', 'char_unk_token', 'char_pad_token', 'char_start_token', 'char_end_token', 'char_maks_token', 'char_unk_token_idx', 'char_pad_token_idx', 'char_start_token_idx', 'char_end_token_idx', 'char_mask_token_idx'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100004"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc[\"token2idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc[\"chartoken2idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_path = PATH.joinpath(\"voc\").absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab_dict(str(voc_path), voc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
